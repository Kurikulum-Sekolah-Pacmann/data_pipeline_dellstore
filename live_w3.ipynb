{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week #3 - Live Class\n",
    "Data Pipeline Course - Sekolah Engineer - Pacmann Academy \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Data Ingestion` emphasizes pulling data from various sources (`Extract`) and directing it to targets (`Load`).\n",
    "- `Data Extraction` involves retrieving data from various sources\n",
    "- `Data Loading` involves transferring this data into target storage systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consideration when carrying out Ingestion\n",
    "- What’s the format? (Unstructured, Semi-structured, Structured)\n",
    "- What’s the frequency? (Batch, Micro-Batch, Streaming)\n",
    "- Is the data ingestion process synchronous or asynchronous?\n",
    "- What is the expected data volume?\n",
    "- What measures are in place to ensure data reliability during ingestion?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Full Extraction: Extracting all the data from the source system\n",
    "- Full Load: Load all the data to destination system\n",
    "- Incremental Extraction: Extracting only the new data or data that has changed since the last extraction\n",
    "- Incremental Load: Load only new or updated data at regular intervals, rather than moving all data at once\n",
    "\n",
    "<img src='pict/live_w3_01.png' width=\"800\"> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study Case: Dell DVD Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Description\n",
    "\n",
    "`Problem`\n",
    "\n",
    "The Dell DVD Store is facing challenges with its current data processing. The store needs to handle data from multiple sources such as spreadsheets, databases, and APIs. The key challenges include:\n",
    "- Database: The Dell DVD Store saves data from current system.\n",
    "- API: Retrieves data from the old system and contains historical data from the old system.\n",
    "- Spreadsheet: Contains analysis results from the team about order status based on the current product stock.\n",
    "\n",
    "<img src='pict/live_w3_03.png' width=\"800\"> <br>\n",
    "\n",
    "`Solution`\n",
    "\n",
    "To address these challenges, we propose creating a comprehensive data pipeline for the Dell DVD Store. This pipeline will involve the following steps:\n",
    "- Data Extraction:\n",
    "Sources: Extract data from spreadsheets, databases, and APIs.\n",
    "Techniques: Use both full and incremental extraction methods to retrieve data efficiently.\n",
    "- Data Load:\n",
    "Staging: Load raw data into a staging database (PostgreSQL) without transformation.\n",
    "Final Load: Transfer clean and transformed data to the final destination.\n",
    "Failure Handling: Log failed data loads to MinIO object storage for reprocessing\n",
    "- Data Transformation:\n",
    "Cleaning: Handle missing values, incorrect data formats, and other data quality issues.\n",
    "Trasnforming: Add derived fields and calculated metrics as needed.\n",
    "\n",
    "`Tools and Technologies`:\n",
    "- Python: For build Data Pipeline\n",
    "- PostgreSQL: For log, staging and final data storage.\n",
    "- MinIO: For load failed data.\n",
    "- Docker: For running MinIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset\n",
    "1. Restore Database Dell DVD Store [Link](https://drive.google.com/drive/folders/1ED0sg2AZNH_Kl5Pb1cBUufnPCphpM21R)\n",
    "2. Dupplicate Spreadsheet [Link](https://drive.google.com/drive/folders/1ED0sg2AZNH_Kl5Pb1cBUufnPCphpM21R)\n",
    "3. Check Data API [Link](https://api-history-order.vercel.app/api/dummydata?start_date=2004-01-01&end_date=2004-01-01)\n",
    "\n",
    "For Porject\n",
    "1. Save Your Credential Google Service Account\n",
    "2. Prepare Your MiniO (Access Key, Secreet Key, Bucket Name: \"error-dellstore\")\n",
    "3. Create Your Database (log and staging)\n",
    "4. create your .env\n",
    "\n",
    "    ```\n",
    "    DB_HOST=\"localhost\"\n",
    "    DB_USER=\"YOUR POSTGRES USER\"\n",
    "    DB_PASS=\"YOUR POSGRES PASS\"\n",
    "\n",
    "    DB_NAME_SOURCE=\"dellstore\"\n",
    "    DB_NAME_STG=\"staging\"\n",
    "    DB_SHCHEMA_STG=\"staging\"\n",
    "    DB_NAME_log=\"etl_log\"\n",
    "\n",
    "    CRED_PATH='YOUR_PATH/creds/data-pipeline-427506-50d868a444ee.json'\n",
    "    MODEL_PATH='YOUR_PATH/models/'\n",
    "    KEY_SPREADSHEET=\"YOUR SPREADSHEET KEY\"\n",
    "\n",
    "    ACCESS_KEY_MINIO = 'YOUR MINIO ACCESS KEY'\n",
    "    SECRET_KEY_MINIO = 'YOUR MINIO SECRET KEY'\n",
    "\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "import gspread\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "import sqlalchemy\n",
    "import requests\n",
    "from pangres import upsert\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load your file .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\".env\")\n",
    "\n",
    "DB_HOST = os.getenv(\"DB_HOST\")\n",
    "DB_USER = os.getenv(\"DB_USER\")\n",
    "DB_PASS = os.getenv(\"DB_PASS\")\n",
    "\n",
    "DB_NAME_SOURCE = os.getenv(\"DB_NAME_SOURCE\")\n",
    "DB_NAME_STG = os.getenv(\"DB_NAME_STG\")\n",
    "DB_SHCHEMA_STG = os.getenv(\"DB_SHCHEMA_STG\")\n",
    "DB_NAME_log = os.getenv(\"DB_NAME_log\")\n",
    "\n",
    "\n",
    "CRED_PATH = os.getenv(\"CRED_PATH\")\n",
    "KEY_SPREADSHEET = os.getenv(\"KEY_SPREADSHEET\")\n",
    "MODEL_PATH = os.getenv(\"MODEL_PATH\")\n",
    "\n",
    "ACCESS_KEY_MINIO = os.getenv(\"ACCESS_KEY_MINIO\")\n",
    "SECRET_KEY_MINIO = os.getenv(\"SECRET_KEY_MINIO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To manage SQL queries efficiently using external `.sql` files, you can create a function that reads these files and returns their content.<br>\n",
    "Each `.sql` file should contain the SQL query for the respective table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sql(table_name):\n",
    "    #open your file .sql\n",
    "    with open(f\"{MODEL_PATH}{table_name}.sql\", 'r') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    #return query text\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Log Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A log is a record of events that occur during the execution of a data pipeline. It captures essential information about the processes and their status, making it easier to monitor, debug, and audit the pipeline operations \n",
    "\n",
    "The etl_log function is designed to log ETL (Extract, Transform, Load) operations into a PostgreSQL database.\n",
    "\n",
    "Your Log Message: <br>\n",
    "<code>\n",
    "log_msg = { <br>\n",
    "            \"step\": \"staging | warehouse\"\n",
    "            \"process\" : \"extraction | transformation | load\", <br>\n",
    "            \"status\": \"success | failed\", <br>\n",
    "            \"source\": \"spreadsheet | database | api\", <br>\n",
    "            \"table_name\": \"worksheet_name | table_name\", <br>\n",
    "            \"etl_date\": \"Current timestamp\" <br>\n",
    "            \"error_msg\": \"Error Message when error occur\" <br>\n",
    "        }\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def etl_log(log_msg: dict):\n",
    "    try:\n",
    "        # create connection to database\n",
    "        conn = create_engine(f\"postgresql://{DB_USER}:{DB_PASS}@{DB_HOST}/{DB_NAME_log}\")\n",
    "        \n",
    "        # convert dictionary to dataframe\n",
    "        df_log = pd.DataFrame([log_msg])\n",
    "\n",
    "        #extract data log\n",
    "        df_log.to_sql(name = \"etl_log\",  # Your log table\n",
    "                        con = conn,\n",
    "                        if_exists = \"append\",\n",
    "                        index = False)\n",
    "    except Exception as e:\n",
    "        print(\"Can't save your log message. Cause: \", str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fuction read_etl_log untuk membaca informasi log dari tabel log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_etl_log(filter_params: dict):\n",
    "    \"\"\"\n",
    "    function read_etl_log that reads log information from the etl_log table and extracts the maximum etl_date for a specific process, step, table name, and status.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # create connection to database\n",
    "        conn = create_engine(f\"postgresql://{DB_USER}:{DB_PASS}@{DB_HOST}/{DB_NAME_log}\")\n",
    "        \n",
    "        # To help with the incremental process, get the etl_date from the relevant process\n",
    "        \"\"\"\n",
    "        SELECT MAX(etl_date)\n",
    "        FROM etl_log \"\n",
    "        WHERE \n",
    "            step = %s and\n",
    "            table_name ilike %s and\n",
    "            status = %s and\n",
    "            process = %s\n",
    "        \"\"\"\n",
    "        query = sqlalchemy.text(read_sql(\"log\"))\n",
    "\n",
    "        # Execute the query with pd.read_sql\n",
    "        df = pd.read_sql(sql=query, con=conn, params=(filter_params,))\n",
    "\n",
    "        #return extracted data\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(\"Can't execute your query. Cause: \", str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Extract Data From Database\n",
    "\n",
    "Extract Data From PostgreSQL\n",
    "\n",
    "- Full Extraction: Initial Load\n",
    "- Incremental Extraction: Get new and updated data\n",
    "\n",
    "Function Steps:\n",
    "1. Establish Database Connection: Connects to a PostgreSQL database named dellstore.\n",
    "2. Read Log Data: Reads existing log data from log.csv to determine the last successful extraction timestamp (etl_date).\n",
    "3. Initial Load or Incremental Extraction:\n",
    "    - If no previous extraction has been recorded (etl_date is empty), set etl_date to '1111-01-01' indicating the initial load.\n",
    "    - Otherwise, retrieve data added since the last successful extraction (etl_date).\n",
    "4. Query Execution: Constructs a SQL query to select all columns from the specified table_name where created_at is greater than etl_date.\n",
    "5. Data Extraction: Executes the SQL query using pd.read_sql to fetch the data into a Pandas DataFrame (df)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_database(table_name: str): \n",
    "    \n",
    "    try:\n",
    "        # create connection to database\n",
    "        conn = create_engine(f\"postgresql://{DB_USER}:{DB_PASS}@{DB_HOST}/{DB_NAME_SOURCE}\")\n",
    "\n",
    "        # Get date from previous process\n",
    "        filter_log = {\"step_name\": \"staging\",\n",
    "                    \"table_name\": table_name,\n",
    "                    \"status\": \"success\",\n",
    "                    \"process\": \"load\"}\n",
    "        etl_date = read_etl_log(filter_log)\n",
    "\n",
    "\n",
    "        # If no previous extraction has been recorded (etl_date is empty), set etl_date to '1111-01-01' indicating the initial load.\n",
    "        # Otherwise, retrieve data added since the last successful extraction (etl_date).\n",
    "        if(etl_date['max'][0] == None):\n",
    "            etl_date = '1111-01-01'\n",
    "        else:\n",
    "            etl_date = etl_date\n",
    "\n",
    "        # Constructs a SQL query to select all columns from the specified table_name where created_at is greater than etl_date.\n",
    "        \"\"\"\n",
    "        SELECT * \n",
    "        FROM customers \n",
    "        WHERE created_at > :etl_date\n",
    "        \"\"\"\n",
    "        query = sqlalchemy.text(read_sql(table_name))\n",
    "\n",
    "        # Execute the query with pd.read_sql\n",
    "        df = pd.read_sql(sql=query, con=conn, params=({\"etl_date\":etl_date},))\n",
    "        log_msg = {\n",
    "                \"step\" : \"staging\",\n",
    "                \"process\":\"extraction\",\n",
    "                \"status\": \"success\",\n",
    "                \"source\": \"database\",\n",
    "                \"table_name\": table_name,\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # Current timestamp\n",
    "            }\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        log_msg = {\n",
    "            \"step\" : \"staging\",\n",
    "            \"process\":\"extraction\",\n",
    "            \"status\": \"failed\",\n",
    "            \"source\": \"database\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),  # Current timestamp\n",
    "            \"error_msg\": str(e)\n",
    "        }\n",
    "    finally:\n",
    "        etl_log(log_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Extract Data From API\n",
    "\n",
    "Extract Data From API (context: Old data from previous system)\n",
    "\n",
    "- Backfilling: Specify a date range in the date parameter to retrieve historical data\n",
    "\n",
    "Function Steps: \n",
    "1. Establish API endpoint: [Link](https://api-history-order.vercel.app)\n",
    "2. List of parameter API\n",
    "    - start_date\n",
    "    - end_date\n",
    "5. Data Extraction: Hit the endpoint API to fetch the data into a Pandas DataFrame (df)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_api(link_api:str, list_parameter:dict, data_name):\n",
    "    try:\n",
    "        # Establish connection to API\n",
    "        resp = requests.get(link_api, params=list_parameter)\n",
    "\n",
    "        # Parse the response JSON\n",
    "        raw_response = resp.json()\n",
    "\n",
    "        # Convert the JSON data to a pandas DataFrame\n",
    "        df_api = pd.DataFrame(raw_response)\n",
    "\n",
    "        # create success log message\n",
    "        log_msg = {\n",
    "                \"step\" : \"staging\",\n",
    "                \"process\":\"extraction\",\n",
    "                \"status\": \"success\",\n",
    "                \"source\": \"api\",\n",
    "                \"table_name\": data_name,\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # Current timestamp\n",
    "            }\n",
    "        return df_api\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred while making the API request: {e}\")\n",
    "\n",
    "        # create fail log message\n",
    "        log_msg = {\n",
    "                \"step\" : \"staging\",\n",
    "                \"process\":\"extraction\",\n",
    "                \"status\": \"failed\",\n",
    "                \"source\": \"api\",\n",
    "                \"table_name\": data_name,\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),  # Current timestamp\n",
    "                \"error_msg\": str(e)\n",
    "            }\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "\n",
    "    except ValueError as e:\n",
    "        print(f\"An error occurred while parsing the response JSON: {e}\")\n",
    "\n",
    "        # create fail log message\n",
    "        log_msg = {\n",
    "                \"step\" : \"staging\",\n",
    "                \"process\":\"extraction\",\n",
    "                \"status\": \"failed\",\n",
    "                \"source\": \"api\",\n",
    "                \"table_name\": data_name,\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),  # Current timestamp\n",
    "                \"error_msg\": str(e)\n",
    "            }\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    finally:\n",
    "        etl_log(log_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Extract Data From Spreadsheet\n",
    "\n",
    "Steps:\n",
    "1. Define the credentials needed to access the spreadsheet.\n",
    "2. Open file by key\n",
    "3. Retrieve data from a specific sheet.\n",
    "4. Perform Data Extraction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function aims to define the credentials needed to access the spreadsheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auth_gspread():\n",
    "    scope = ['https://spreadsheets.google.com/feeds',\n",
    "             'https://www.googleapis.com/auth/drive']\n",
    "\n",
    "    #Define your credentials\n",
    "    credentials = ServiceAccountCredentials.from_json_keyfile_name(CRED_PATH, scope) # Your json file here\n",
    "\n",
    "    gc = gspread.authorize(credentials)\n",
    "\n",
    "    return gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key_file is a unique identifier for the Google Sheets file.\n",
    "Access to the spreadsheet file is obtained using the key from that file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_key_file(key_file:str):\n",
    "    #define credentials to open the file\n",
    "    gc = auth_gspread()\n",
    "    \n",
    "    #open spreadsheet file by key\n",
    "    sheet_result = gc.open_by_key(key_file)\n",
    "    \n",
    "    return sheet_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extract_spreadsheet function is used to retrieve data from a specific sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sheet(key_file:str, worksheet_name: str) -> pd.DataFrame:\n",
    "    # init sheet\n",
    "    sheet_result = init_key_file(key_file)\n",
    "    \n",
    "    worksheet_result = sheet_result.worksheet(worksheet_name)\n",
    "    \n",
    "    df_result = pd.DataFrame(worksheet_result.get_all_values())\n",
    "    \n",
    "    # set first rows as columns\n",
    "    df_result.columns = df_result.iloc[0]\n",
    "    \n",
    "    # get all the rest of the values\n",
    "    df_result = df_result[1:].copy()\n",
    "    \n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Perform Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_spreadsheet(worksheet_name: str, key_file: str):\n",
    "\n",
    "    try:\n",
    "        # extract data\n",
    "        df_data = extract_sheet(worksheet_name = worksheet_name,\n",
    "                                    key_file = key_file)\n",
    "        \n",
    "        # success log message\n",
    "        log_msg = {\n",
    "                \"step\" : \"staging\",\n",
    "                \"process\":\"extraction\",\n",
    "                \"status\": \"success\",\n",
    "                \"source\": \"spreadsheet\",\n",
    "                \"table_name\": worksheet_name,\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # Current timestamp\n",
    "            }\n",
    "    except Exception as e:\n",
    "        # fail log message\n",
    "        log_msg = {\n",
    "                \"step\" : \"staging\",\n",
    "                \"process\":\"extraction\",\n",
    "                \"status\": \"failed\",\n",
    "                \"source\": \"spreadsheet\",\n",
    "                \"table_name\": worksheet_name,\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),  # Current timestamp\n",
    "                \"error_msg\": str(e)\n",
    "            }\n",
    "    finally:\n",
    "        # load log to csv file\n",
    "        etl_log(log_msg)\n",
    "        \n",
    "    return df_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Data Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Handle Failure Data\n",
    "\n",
    "we will learn how to handle any data failures processing by storing the failed data in object storage using MinIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Minio libray is used to interact with a MinIO server. \n",
    "from minio import Minio\n",
    "\n",
    "# BytesIO provides a way to work with binary data in memory as if it were a file.\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Function handle_error to dump failure data to MiniO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_error(data, bucket_name:str, table_name:str):\n",
    "\n",
    "    current_date = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    # Initialize MinIO client\n",
    "    client = Minio('localhost:9000',\n",
    "                access_key=ACCESS_KEY_MINIO,\n",
    "                secret_key=SECRET_KEY_MINIO,\n",
    "                secure=False)\n",
    "\n",
    "    # Make a bucket if it doesn't exist\n",
    "    if not client.bucket_exists(bucket_name):\n",
    "        client.make_bucket(bucket_name)\n",
    "\n",
    "    # Convert DataFrame to CSV and then to bytes\n",
    "    csv_bytes = data.to_csv().encode('utf-8')\n",
    "    csv_buffer = BytesIO(csv_bytes)\n",
    "\n",
    "    # Upload the CSV file to the bucket\n",
    "    client.put_object(\n",
    "        bucket_name=bucket_name,\n",
    "        object_name=f\"{table_name}_{current_date}.csv\", #name the fail source name and current etl date\n",
    "        data=csv_buffer,\n",
    "        length=len(csv_bytes),\n",
    "        content_type='application/csv'\n",
    "    )\n",
    "\n",
    "    # List objects in the bucket\n",
    "    objects = client.list_objects(bucket_name, recursive=True)\n",
    "    for obj in objects:\n",
    "        print(obj.object_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Load Data to Satging Area\n",
    "- Load raw data to staging area\n",
    "- strategy: apply upsert using libray pangres based on primary key for each data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_staging(data, schema:str, table_name: str, idx_name:str, source):\n",
    "    try:\n",
    "        # create connection to database\n",
    "        conn = create_engine(f\"postgresql://{DB_USER}:{DB_PASS}@{DB_HOST}/{DB_NAME_STG}\")\n",
    "        \n",
    "        # set data index or primary key\n",
    "        data = data.set_index(idx_name)\n",
    "        \n",
    "        # Do upsert (Update for existing data and Insert for new data)\n",
    "        upsert(con = conn,\n",
    "                df = data,\n",
    "                table_name = table_name,\n",
    "                schema = schema,\n",
    "                if_row_exists = \"update\")\n",
    "        \n",
    "        #create success log message\n",
    "        log_msg = {\n",
    "                \"step\" : \"staging\",\n",
    "                \"process\":\"load\",\n",
    "                \"status\": \"success\",\n",
    "                \"source\": source,\n",
    "                \"table_name\": table_name,\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # Current timestamp\n",
    "            }\n",
    "        # return data\n",
    "    except Exception as e:\n",
    "\n",
    "        #create fail log message\n",
    "        log_msg = {\n",
    "            \"step\" : \"staging\",\n",
    "            \"process\":\"load\",\n",
    "            \"status\": \"failed\",\n",
    "            \"source\": source,\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") , # Current timestamp\n",
    "            \"error_msg\": str(e)\n",
    "        }\n",
    "\n",
    "        # Handling error: save data to Object Storage\n",
    "        try:\n",
    "            handle_error(data = data, bucket_name='error-dellstore', table_name= table_name)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    finally:\n",
    "        etl_log(log_msg)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src='pict/live_w3_04.png' width=\"800\"> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Ingestion Dell DVD Store Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schema Source: <br>\n",
    "<img src='pict/live_w3_05.png' width=\"500\"> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract and Load Data Customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract\n",
    "df_customer = extract_database(table_name = \"customers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customerid</th>\n",
       "      <th>firstname</th>\n",
       "      <th>lastname</th>\n",
       "      <th>address1</th>\n",
       "      <th>address2</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zip</th>\n",
       "      <th>country</th>\n",
       "      <th>region</th>\n",
       "      <th>...</th>\n",
       "      <th>phone</th>\n",
       "      <th>creditcardtype</th>\n",
       "      <th>creditcard</th>\n",
       "      <th>creditcardexpiration</th>\n",
       "      <th>username</th>\n",
       "      <th>password</th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "      <th>gender</th>\n",
       "      <th>created_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>Becky</td>\n",
       "      <td>Cochran</td>\n",
       "      <td>193 Hailey Views\\nMichaelside, AS 48241</td>\n",
       "      <td>None</td>\n",
       "      <td>East Charleneshire</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>53868</td>\n",
       "      <td>US</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2415449050</td>\n",
       "      <td>5</td>\n",
       "      <td>6630987872369588</td>\n",
       "      <td>2010/03</td>\n",
       "      <td>beckycochran123</td>\n",
       "      <td>password</td>\n",
       "      <td>58</td>\n",
       "      <td>60000</td>\n",
       "      <td>M</td>\n",
       "      <td>2002-03-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>Raymond</td>\n",
       "      <td>Yang</td>\n",
       "      <td>683 Albert Ports\\nLake Waltershire, CO 77913</td>\n",
       "      <td>None</td>\n",
       "      <td>Laneberg</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>18452</td>\n",
       "      <td>US</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1896033667</td>\n",
       "      <td>2</td>\n",
       "      <td>3715867913328111</td>\n",
       "      <td>2011/10</td>\n",
       "      <td>raymondyang123</td>\n",
       "      <td>password</td>\n",
       "      <td>27</td>\n",
       "      <td>20000</td>\n",
       "      <td>F</td>\n",
       "      <td>2003-10-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>Melanie</td>\n",
       "      <td>Wade</td>\n",
       "      <td>514 Tonya Heights Suite 730\\nSouth Davidfurt, ...</td>\n",
       "      <td>None</td>\n",
       "      <td>Port Jessica</td>\n",
       "      <td>Delaware</td>\n",
       "      <td>53356</td>\n",
       "      <td>US</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3029418206</td>\n",
       "      <td>5</td>\n",
       "      <td>3617457962129265</td>\n",
       "      <td>2009/11</td>\n",
       "      <td>melaniewade123</td>\n",
       "      <td>password</td>\n",
       "      <td>43</td>\n",
       "      <td>100000</td>\n",
       "      <td>M</td>\n",
       "      <td>2001-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>Heather</td>\n",
       "      <td>Cruz</td>\n",
       "      <td>75935 Flynn Island Suite 933\\nSouth Alexis, MD...</td>\n",
       "      <td>None</td>\n",
       "      <td>Maryton</td>\n",
       "      <td>North Dakota</td>\n",
       "      <td>44395</td>\n",
       "      <td>US</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3748672054</td>\n",
       "      <td>4</td>\n",
       "      <td>3344003576319665</td>\n",
       "      <td>2011/07</td>\n",
       "      <td>heathercruz123</td>\n",
       "      <td>password</td>\n",
       "      <td>85</td>\n",
       "      <td>80000</td>\n",
       "      <td>M</td>\n",
       "      <td>2003-07-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>Heather</td>\n",
       "      <td>Burgess</td>\n",
       "      <td>6732 Brandi Trafficway Suite 104\\nNorth Jonath...</td>\n",
       "      <td>None</td>\n",
       "      <td>South Ryan</td>\n",
       "      <td>New Hampshire</td>\n",
       "      <td>37471</td>\n",
       "      <td>US</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3354132892</td>\n",
       "      <td>4</td>\n",
       "      <td>8717996907886119</td>\n",
       "      <td>2008/05</td>\n",
       "      <td>heatherburgess123</td>\n",
       "      <td>password</td>\n",
       "      <td>66</td>\n",
       "      <td>100000</td>\n",
       "      <td>M</td>\n",
       "      <td>2000-05-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   customerid firstname lastname  \\\n",
       "0          11     Becky  Cochran   \n",
       "1          12   Raymond     Yang   \n",
       "2          13   Melanie     Wade   \n",
       "3          14   Heather     Cruz   \n",
       "4          15   Heather  Burgess   \n",
       "\n",
       "                                            address1 address2  \\\n",
       "0            193 Hailey Views\\nMichaelside, AS 48241     None   \n",
       "1       683 Albert Ports\\nLake Waltershire, CO 77913     None   \n",
       "2  514 Tonya Heights Suite 730\\nSouth Davidfurt, ...     None   \n",
       "3  75935 Flynn Island Suite 933\\nSouth Alexis, MD...     None   \n",
       "4  6732 Brandi Trafficway Suite 104\\nNorth Jonath...     None   \n",
       "\n",
       "                 city          state    zip country  region  ...       phone  \\\n",
       "0  East Charleneshire   Pennsylvania  53868      US       1  ...  2415449050   \n",
       "1            Laneberg   Pennsylvania  18452      US       1  ...  1896033667   \n",
       "2        Port Jessica       Delaware  53356      US       1  ...  3029418206   \n",
       "3             Maryton   North Dakota  44395      US       1  ...  3748672054   \n",
       "4          South Ryan  New Hampshire  37471      US       1  ...  3354132892   \n",
       "\n",
       "  creditcardtype        creditcard creditcardexpiration           username  \\\n",
       "0              5  6630987872369588              2010/03    beckycochran123   \n",
       "1              2  3715867913328111              2011/10     raymondyang123   \n",
       "2              5  3617457962129265              2009/11     melaniewade123   \n",
       "3              4  3344003576319665              2011/07     heathercruz123   \n",
       "4              4  8717996907886119              2008/05  heatherburgess123   \n",
       "\n",
       "   password age  income  gender created_at  \n",
       "0  password  58   60000       M 2002-03-01  \n",
       "1  password  27   20000       F 2003-10-01  \n",
       "2  password  43  100000       M 2001-11-01  \n",
       "3  password  85   80000       M 2003-07-01  \n",
       "4  password  66  100000       M 2000-05-01  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_customer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load\n",
    "load_staging(data = df_customer.iloc[:, :-1], schema=\"staging\",\n",
    "             table_name=\"customers\", idx_name=\"customerid\",\n",
    "             source=\"database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract and Load Data Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract\n",
    "df_categories = extract_database(table_name = \"categories\")\n",
    "df_categories.head()\n",
    "\n",
    "# Load\n",
    "load_staging(data = df_categories.iloc[:, :-1], schema=\"staging\",\n",
    "             table_name=\"categories\", idx_name=\"category\",\n",
    "             source=\"database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract and Load Data Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract\n",
    "df_products = extract_database(table_name = \"products\")\n",
    "df_products.head()\n",
    "\n",
    "# Load\n",
    "load_staging(data = df_products.iloc[:, :-1], schema=\"staging\",\n",
    "             table_name=\"products\", idx_name=\"prod_id\",\n",
    "             source=\"database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract and Load Data Inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract\n",
    "df_inventory = extract_database(table_name = \"inventory\")\n",
    "df_inventory.head()\n",
    "\n",
    "# Load\n",
    "load_staging(data = df_inventory.iloc[:, :-1], schema=\"staging\",\n",
    "             table_name=\"inventory\", idx_name=\"prod_id\",\n",
    "             source=\"database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract and Load Data Orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract\n",
    "df_orders = extract_database(table_name = \"orders\")\n",
    "df_orders.head()\n",
    "\n",
    "# Load\n",
    "load_staging(data = df_orders.iloc[:, :-1], schema=\"staging\",\n",
    "             table_name=\"orders\", idx_name=\"orderid\",\n",
    "             source=\"database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract and Load Data Orderlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract\n",
    "df_orderlines = extract_database(table_name = \"orderlines\")\n",
    "df_orderlines.head()\n",
    "\n",
    "# Load\n",
    "load_staging(data = df_orderlines.iloc[:, :-1], schema=\"staging\",\n",
    "             table_name=\"orderlines\", idx_name=[\"orderid\",\"orderlineid\"],\n",
    "             source=\"database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract and Load Data Customer History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract\n",
    "df_cust_hist = extract_database(table_name = \"cust_hist\")\n",
    "df_cust_hist.head()\n",
    "\n",
    "# Load\n",
    "load_staging(data = df_cust_hist.iloc[:, :-1], schema=\"staging\",\n",
    "             table_name=\"cust_hist\", idx_name=['customerid','orderid','prod_id'],\n",
    "             source=\"database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Ingestion Data History API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Data API contains data for the period between '2004-01-01' and '2004-02-29'. \n",
    "\n",
    "The API cannot send large amounts of data in a single request, so iterate through the data on a daily basis from '2004-01-01' to '2004-02-29'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Function to create list of date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a list of date strings\n",
    "def date_range(start_date, end_date):\n",
    "    delta = end_date - start_date\n",
    "    return [(start_date + timedelta(days=i)).strftime(\"%Y-%m-%d\") for i in range(delta.days + 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create date range  between '2004-01-01' and '2004-02-29'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the date range\n",
    "start_date = datetime.strptime(\"2004-01-01\", \"%Y-%m-%d\")\n",
    "end_date = datetime.strptime(\"2004-02-29\", \"%Y-%m-%d\")\n",
    "\n",
    "# Generate list of dates\n",
    "dates = date_range(start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_api = \"https://api-history-order.vercel.app/api/dummydata\"\n",
    "\n",
    "# Iterate over each day and extract data\n",
    "for date in dates:\n",
    "    list_parameter = {\n",
    "        \"start_date\": date,\n",
    "        \"end_date\": date,\n",
    "    }\n",
    "\n",
    "    # Extract Data\n",
    "    df_backfilling = extract_api(link_api, list_parameter, \"customer_orders_history\")\n",
    "    \n",
    "    #Load Data\n",
    "    if(not df_backfilling.empty):\n",
    "        load_staging(data = df_backfilling, schema=\"staging\",\n",
    "                table_name=\"customer_orders_history\", idx_name=['customer_id','order_id','orderline_id'],\n",
    "                source=\"api\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Ingestion Data Spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract\n",
    "df_analytic = extract_spreadsheet(worksheet_name = 'dellstore_analytic',\n",
    "                                    key_file = KEY_SPREADSHEET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orderid</th>\n",
       "      <th>sum_stock</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6114</td>\n",
       "      <td>0</td>\n",
       "      <td>fulfilled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11233</td>\n",
       "      <td>100</td>\n",
       "      <td>backordered</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4790</td>\n",
       "      <td>0</td>\n",
       "      <td>fulfilled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>273</td>\n",
       "      <td>0</td>\n",
       "      <td>fulfilled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11719</td>\n",
       "      <td>0</td>\n",
       "      <td>fulfilled</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "0 orderid sum_stock       status\n",
       "1    6114         0    fulfilled\n",
       "2   11233       100  backordered\n",
       "3    4790         0    fulfilled\n",
       "4     273         0    fulfilled\n",
       "5   11719         0    fulfilled"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_analytic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load\n",
    "load_staging(data = df_analytic, schema=\"staging\",\n",
    "                table_name=\"order_status_analytic\", idx_name=\"orderid\",\n",
    "                source=\"spreadsheet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "link git repository: [git repository](https://github.com/Kurikulum-Sekolah-Pacmann/data_pipeline_dellstore.git)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
